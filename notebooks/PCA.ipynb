{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecfdfb28",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "Run the following setup script first. Change `remote_data_dir` to the name of the desired data folder in Solitabay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee8c114",
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_data_dir = 'raw_audio_data_20211007'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c18692",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "if 'google.colab' in str(get_ipython()) and not Path('/content/data').is_dir(): # we only need to run this once\n",
    "    !wget -q -O /content/setup.sh https://raw.githubusercontent.com/solita/ivves-machine-spraak/main/setup.sh\n",
    "    !bash /content/setup.sh $remote_data_dir\n",
    "else:\n",
    "    print('This notebook is only meant to be run in Google Colab.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65bc45a-b458-4ef5-9334-145e3079f2ac",
   "metadata": {},
   "source": [
    "# PCA and Clustering\n",
    "\n",
    "In this notebook we'll investigate the effectiveness of applying PCA (based on a single or multiple input files and for varying window lengths) to the audio data and clustering based on the strengths of the filtered signals by using the k-means algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951f8a20-9c9c-40e3-9ea9-342ce85e11fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "from math import ceil\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import modules.utils as utl\n",
    "import modules.pca_clustering as pcc\n",
    "\n",
    "data_folder = Path('/content/data/converted/')\n",
    "sample_rate, samples, names = utl.load_data(data_folder, channel=0)\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5728f3f3-7edc-492c-913b-a4aac65f1045",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Analysis\n",
    "\n",
    "### Window length\n",
    "\n",
    "We first set the `window_length` which corresponds to how many sequential time steps we use as an input to PCA. The effect of this is essentially to downsample the signal into chunks spanning a longer period of time (but to still use the full data to fit the PCA). This introduces a local time dependency into each principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8fbccf-f0bc-45cc-a5e3-e751cae6a6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_length = 25\n",
    "stride = 25\n",
    "wdw = signal.get_window('boxcar', window_length)\n",
    "plt.plot(wdw)\n",
    "plt.title('Window for PCA inputs')\n",
    "plt.xlabel('Frames')\n",
    "plt.ylabel('Weight')\n",
    "plt.show()\n",
    "\n",
    "f, t, s = pcc.sample2spec(np.array([samples[0]]), sample_rate)\n",
    "# Note that len(samples[0]) / sample_rate ~ ts[0][-1] = length of clip\n",
    "print(f'Total length of PCA window = {1000 * len(wdw) * (len(samples[0]) / sample_rate) / len(t[0]):.2f} ms '\n",
    "      + f'with strides of {1000 * stride * (len(samples[0]) / sample_rate) / len(t[0]):.2f} ms.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd674c44-880f-4ec1-85c8-802005ef8a34",
   "metadata": {},
   "source": [
    "### Applying PCA\n",
    "\n",
    "First we visualise the principal components based on a single audio sample. The color corresponds to the coefficient of each principal component in the projection of the data in a given time interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0984b5a0-bf0e-4424-8a70-df535a7fe207",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, t, s = pcc.sample2spec(np.array([samples[7]]), sample_rate)\n",
    "spec_pca, _, _ = pcc.spec2pca(s, wdw, stride=stride, n_components=10)\n",
    "pcc.plot_pca(spec_pca[0], t[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5cd53b-7589-4dec-bee0-ca29d857b238",
   "metadata": {},
   "source": [
    "We can first try to use the above principal components of a single sample (defined by `sample_to_use`) to decompose all the other samples. The source sample is highlighted in the below plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ff0063-58be-466d-a194-7322de110b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_to_use = 7\n",
    "\n",
    "fs, ts, sxxs = pcc.sample2spec(samples, sample_rate)\n",
    "# fit the pca to a single sample\n",
    "_, pca, sclr = pcc.spec2pca([sxxs[sample_to_use]], wdw, stride=stride, n_components=10)\n",
    "spec_pcas, _, _ = pcc.spec2pca(sxxs, wdw, stride=stride, pca=pca, sclr=sclr)\n",
    "\n",
    "# clip the values to aid visualisation\n",
    "#for i in range(len(spec_pcas)):\n",
    "#    spec_pcas[i] = np.clip(spec_pcas[i], -100, 100)\n",
    "\n",
    "fig = pcc.plot_pca_grid(spec_pcas, ts, stride=stride, names=names, title='PCA based on a single clip',\n",
    "                        hl_idx=[sample_to_use], equal_scale=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017a9ca1-2e04-4680-993c-b6102a999c60",
   "metadata": {},
   "source": [
    "For comparison, here's PCA performed independently on each individual sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f0af4f-65b3-4a98-86fc-b362c4b341a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs, ts, sxxs = pcc.sample2spec(samples, sample_rate)\n",
    "# fit the pca to a single sample\n",
    "spec_pcas = []\n",
    "for sxx in sxxs:\n",
    "    (sp,), _, _ = pcc.spec2pca([sxx], wdw, stride=stride, n_components=10)\n",
    "    spec_pcas.append(sp)\n",
    "    \n",
    "#for i in range(len(spec_pcas)):\n",
    "#    spec_pcas[i] = np.clip(spec_pcas[i], -100, 100)\n",
    "    \n",
    "pcc.plot_pca_grid(spec_pcas, ts, stride=stride, names=names, title='PCA independently on each clip',\n",
    "                  equal_scale=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556c841f-9f31-4b8c-86e6-0c2892e7ee6e",
   "metadata": {},
   "source": [
    "Finally, the most sensible thing to do (possibly?) is to perform PCA on all the chunks from the 8 different audio clips simultaneously (so that for each clip the principal components have the same \"meaning\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d701ef-e8db-4f94-adfb-609614f51210",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs, ts, sxxs = pcc.sample2spec(samples, sample_rate)\n",
    "spec_pcas, _, _ = pcc.spec2pca(sxxs, wdw, stride=stride, n_components=10)\n",
    "\n",
    "#for i in range(len(spec_pcas)):\n",
    "#    spec_pcas[i] = np.clip(spec_pcas[i], -100, 100)  \n",
    "    \n",
    "pcc.plot_pca_grid(spec_pcas, ts, stride=stride, names=names, title='PCA on all clips simultaneously',\n",
    "                  equal_scale=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9797121-d512-4438-88ec-953bba5fe845",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcc.plot_pca(spec_pcas[7], ts[7], halfrange=148.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b3b799-9823-42bd-806d-4e7c10098d5b",
   "metadata": {},
   "source": [
    "We can see that the first principal component heavily dominates compared to the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd78526-9525-4ea3-8706-b10a7a0caf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(1, len(pca.explained_variance_ratio_)+1), pca.explained_variance_ratio_)\n",
    "plt.title('% of variance explained by each PC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0effd54e-96fc-4553-9c9a-02bf2cec51c2",
   "metadata": {},
   "source": [
    "## Loadings\n",
    "\n",
    "Loadings are scaled eigenvectors of the covariance matrix $\\frac{1}{N-1}X^{\\mathsf{T}}X$ corresponding to the design matrix $X\\in\\mathbb{R}^{N\\times p}$. The scaling is given by the square root of the respective eigenvalues, which are also the singular values of $X$ in the SVD decomposition $X=USV^{\\mathsf{T}}$. Thus we can write the loading matrix $L$ as $$L=\\frac{1}{\\sqrt{N-1}}VS.$$\n",
    "Since the eigenvectors are stored in `pca.components_` and the eigenvalues in `pca.explained_variance_`, we see that\n",
    "```\n",
    "loadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n",
    "```\n",
    "For a short summary see <https://stats.stackexchange.com/questions/104306/what-is-the-difference-between-loadings-and-correlation-loadings-in-pca-and/104640#104640>.\n",
    "\n",
    "**Note:** *There seems to be some confusion about the terminology depending on the source (e.g. the documentation in R refers to the eigenvectors as loadings), so bear that in mind.*\n",
    "\n",
    "When reducing the dimensionality of the data with `pca.transform` it projects to the eigenvectors (as expected; see <https://github.com/scikit-learn/scikit-learn/blob/0d378913b/sklearn/decomposition/_base.py#L97>)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a215d321-7240-46d0-b69d-3eb7c06453cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Visualising individual principal components\n",
    "\n",
    "In general, when we visualise/compare principal components it makes more sense to look at the loadings. This is because in PCA the eigenvectors have norm 1 and as such don't represent quantities in the same scale as the original data (i.e. they are direction vectors). The loading matrix $L$, on the other hand, is precisely the cross-covariance matrix of $X$ and the principal components and so it has a physical meaning.\n",
    "\n",
    "If we apply PCA with `window_length=1` then each principal component is some linear combination of distinct frequencies. On the other hand, for `window_length>1`, we have multiple dimensions in the input space that correspond to the same physical frequency (but at a time offset). We visualise both of these cases below.\n",
    "\n",
    "#### window_length = 1\n",
    "\n",
    "We show the filter (as a wave) corresponding to each principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e965ab8-6b8d-4d6f-808f-b487c18a7244",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_window = signal.get_window('boxcar', 1)\n",
    "fs, ts, sxxs = pcc.sample2spec(samples, sample_rate)\n",
    "spec_pcas, pca, sclr = pcc.spec2pca(sxxs, short_window, stride=1, n_components=10)\n",
    "\n",
    "loadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n",
    "v = np.zeros((loadings.shape[1], len(fs[0])))\n",
    "for i, _ in enumerate(loadings.T):\n",
    "    v[i, :] = loadings[:, i].reshape(len(short_window), v.shape[1])\n",
    "\n",
    "fig, axs = plt.subplots(3, 2, figsize=(16,10), gridspec_kw={'hspace':0.3})\n",
    "for n, ax in enumerate(axs.flat):\n",
    "    ax.bar(fs[0], v[n,:], width=100)\n",
    "    ax.set_title(f'PC{n+1}', fontdict={'fontweight':'bold'}, y=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896e0a91-ed7d-469f-b8af-8693448b7880",
   "metadata": {},
   "source": [
    "#### window_length > 1\n",
    "\n",
    "On the other hand, for larger `window_length` we can show the time evolution of the filter corresponding to a single principal component (or we could just take the average to obtain a single plot per principal component; maybe some other way to visualise?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9404f7fd-81d9-483e-af33-e2c92e06ec12",
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps_to_show = 6\n",
    "long_window = signal.get_window('boxcar', 25)\n",
    "\n",
    "fs, ts, sxxs = pcc.sample2spec(samples, sample_rate)\n",
    "spec_pcas, pca, sclr = pcc.spec2pca(sxxs, long_window, stride=len(long_window), n_components=10)\n",
    "\n",
    "wl = len(long_window)\n",
    "loadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n",
    "v = np.zeros((loadings.shape[1], wl, len(fs[0])))\n",
    "for i, _ in enumerate(loadings.T):\n",
    "    v[i, :, :] = loadings[:, i].reshape(wl, v.shape[-1])\n",
    "\n",
    "width=100\n",
    "\n",
    "fig, axs = plt.subplots(3, 2, figsize=(16,10), gridspec_kw={'hspace':0.3})\n",
    "for n, ax in enumerate(axs.flat):\n",
    "    if n >= wl or n >= timesteps_to_show:\n",
    "        break\n",
    "    ax.bar(fs[0], v[0, n, :], width=width)\n",
    "    ax.set_title(f'PC{1}_{n+1}', fontdict={'fontweight':'bold'}, y=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a264d0-709f-47cf-b374-1ff12d18215d",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "We'll now apply simple k-means clustering to our data and plot the resulting reduced spectrograms. For each time step we also signify the corresponding cluster label at the bottom of each plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ebbce1-9ec2-4795-9cc7-02aa6b0db53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_length = 25\n",
    "stride=25\n",
    "n_clusters = 4\n",
    "n_components = 6\n",
    "\n",
    "spec_pcas, ts, _, kmeans, pca, kmeans_sclr, _ = pcc.cluster(samples, sample_rate, wdw, stride=stride,\n",
    "                                                  n_clusters=n_clusters, n_components=n_components,\n",
    "                                                  max_iter=300, verbose=False)\n",
    "\n",
    "labels = [np.repeat(kmeans.predict(kmeans_sclr.transform(s.T)), repeats=stride) for s in spec_pcas]\n",
    "\n",
    "fig = pcc.plot_pca_grid(spec_pcas, ts, stride=stride, names=names, title='K-means clustering on PCA',\n",
    "                        equal_scale=True)\n",
    "\n",
    "# setup the colors for labels\n",
    "c=cm.Accent\n",
    "colors=[]\n",
    "color_labels = np.arange(n_clusters)\n",
    "for i in range(n_clusters):\n",
    "    colors.append(c.colors[i])\n",
    "tag_cmap = ListedColormap(colors)\n",
    "loc = np.linspace(0, 1, n_clusters+1)\n",
    "loc = loc + 1/(2*n_clusters)\n",
    "\n",
    "for n, ax in enumerate(fig.axes[:-1]):\n",
    "    ax.pcolormesh(ts[n], np.array([0,0.5]), labels[n][np.newaxis, :len(ts[n])-1], shading='flat', cmap=tag_cmap,\n",
    "                 vmin=0, vmax=n_clusters)\n",
    "    ax.axhline(0.5, 0, 1, color='k', linewidth=1)\n",
    "\n",
    "fig.subplots_adjust(right=0.93)\n",
    "cbar_ax = fig.add_axes([0.95, 0.2/3, 0.01, 0.4])\n",
    "cb = fig.colorbar(cm.ScalarMappable(norm=matplotlib.colors.Normalize(vmin=0,vmax=1.), cmap=tag_cmap), cax=cbar_ax)\n",
    "cb.set_ticks(loc)\n",
    "cb.set_ticklabels(color_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f85fcb9-3dc9-406b-9dad-3245897273c3",
   "metadata": {},
   "source": [
    "Here we can zoom in on an indivual clip given by `clip_no`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd691a0-7fab-4974-8986-bbe2a4aaefc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_no = 7\n",
    "\n",
    "fig, ax = pcc.plot_pca(spec_pcas[clip_no], ts[clip_no], stride=stride)\n",
    "# Note the -1 in the range of tags, this is because of shading='flat'\n",
    "p = ax.pcolormesh(ts[clip_no], np.array([0,0.5]), labels[clip_no][np.newaxis, :len(ts[clip_no])-1], shading='flat', cmap=tag_cmap,\n",
    "                  vmin=0, vmax=n_clusters)\n",
    "ax.axhline(0.5, 0, 1, color='k', linewidth=1)\n",
    "ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "cb = fig.colorbar(cm.ScalarMappable(norm=matplotlib.colors.Normalize(vmin=0,vmax=1.), cmap=tag_cmap))\n",
    "cb.set_ticks(loc)\n",
    "cb.set_ticklabels(color_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9707cda8-099a-4c07-bb08-8325ca2630d3",
   "metadata": {},
   "source": [
    "### Elbow Method\n",
    "\n",
    "To check on the effectiveness of the k-means algorithm, we look at the plot of *inertia* as a function of the number of clusters. Suppose the k-means algorithm has produced $n$ clusters $\\mathcal{C}_{1},\\ldots,\\mathcal{C}_{n}$ with centroids $c_{1}, \\ldots, c_{n}$, respectively. Then the inertia of this configuration is defined as\n",
    "$$I=\\sum_{i=1}^{n}\\sum_{x\\in\\mathcal{C}_{i}}|x-c_{i}|^{2},$$\n",
    "where we assume that each data point has been assigned to a cluster with minimal distance to its centroid. Notice that this is precisely the quantity that the k-means algorithm tries to minimise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f10720-f64a-4459-9f31-64545b512585",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_k = 10\n",
    "inertias = []\n",
    "for k in range(2, max_k):\n",
    "    spec_pcas, ts, _, kmeans, _, _, _ = pcc.cluster(samples, sample_rate, wdw, stride=stride,\n",
    "                                                    n_clusters=k, n_components=n_components,\n",
    "                                                    max_iter=300, verbose=False)\n",
    "    inertias.append((k, kmeans.inertia_))\n",
    "    \n",
    "plt.plot(*zip(*inertias))\n",
    "plt.ylabel('Inertia')\n",
    "plt.xlabel('$k$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2654fba-eb32-4b0e-95e2-07118e26ae21",
   "metadata": {},
   "source": [
    "We notice that there is no clear \"elbow\" in the curve, which usually signifies that the data doesn't cluster that nicely and there is no clear optimal number of clusters.\n",
    "\n",
    "Another metric for evaluating the performance of a clustering algorithm is the *silhouette score*. As per the [sklearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html), a silhouette score for a single data point $x$ is defined as\n",
    "$$s(x) = \\frac{b(x)-a(x)}{\\max(a(x), b(x))},$$\n",
    "where $a(x)$ is the mean distance of $x$ to all the data points in its own (i.e. nearest) cluster and $b(x)$ is the minimal mean distance to the points of another cluster than that in $a(x)$. Note that $-1\\leq s(x)\\leq 1$, where a value of 1 signifies a perfectly defined cluster. To obtain the final silhouette score, we average $s(x)$ over the whole dataset. See also [here](https://en.wikipedia.org/wiki/Silhouette_(clustering))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba073801-57dc-4f9d-821f-eaae54d93d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "max_k = 10\n",
    "silhouettes = []\n",
    "for k in range(2, max_k):\n",
    "    spec_pcas, ts, X, kmeans, _, _, _ = pcc.cluster(samples, sample_rate, wdw, stride=stride,\n",
    "                                                    n_clusters=k, n_components=n_components,\n",
    "                                                    max_iter=300, verbose=False)\n",
    "    silhouettes.append((k, silhouette_score(X, kmeans.labels_, metric='euclidean')))\n",
    "    \n",
    "plt.plot(*zip(*silhouettes))\n",
    "plt.ylabel('Silhouette score')\n",
    "plt.xlabel('$k$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5d69c1-1789-4946-8138-db048b767c1c",
   "metadata": {},
   "source": [
    "Based on this plot it seems that 3 or 5 are the most optimal number of clusters according to the silhouette score. By inspecting the plots for the actual clusters, we can see that with $k=3$, the k-means algorithm typically associates the \"anomalies\" to 2 separate clusters while all of the remaining parts are packed into a single cluster. This is not so useful if we want to be able to perform segmentation based on a single clustering result. On the other hand, while $k=4$ obtains a much lower silhouette score, it is actually able to detect whether the machine is in fact running or not. (*note your results could vary a lot since this implementation is not deterministic*)\n",
    "\n",
    "## Train & Dev sets\n",
    "\n",
    "Despite our small sample size, let's investigate how our algorithms perform if we use 6 clips as our training set and the two remaining ones as the dev set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cfbb3a-dc7d-40e1-85e0-dc5d8299947c",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_length = 25\n",
    "stride = 25\n",
    "wdw = signal.get_window('boxcar', window_length)\n",
    "n_components = 10\n",
    "n_clusters = 4\n",
    "normalise = True\n",
    "clip = True\n",
    "\n",
    "s_train = samples[:6]\n",
    "s_dev = samples[6:]\n",
    "\n",
    "\n",
    "spec_pcas_train, ts_train, _, kmeans, pca, kmeans_sclr, pca_sclr = pcc.cluster(s_train, sample_rate, wdw, stride=stride,\n",
    "                                                                               n_clusters=n_clusters, n_components=n_components)\n",
    "fs, ts_dev, Sxx_dev = pcc.sample2spec(s_dev, sample_rate)\n",
    "spec_pcas_dev, _, _ = pcc.spec2pca(Sxx_dev, wdw, stride=stride, sclr=pca_sclr, pca=pca)\n",
    "spec_pcas = spec_pcas_train + spec_pcas_dev\n",
    "ts = ts_train + ts_dev\n",
    "\n",
    "labels = [np.repeat(kmeans.predict(kmeans_sclr.transform(s.T)), repeats=stride) for s in spec_pcas]\n",
    "\n",
    "fig = pcc.plot_pca_grid(spec_pcas, ts, stride=stride, names=names, title='Train/Test PCA & Clustering', hl_idx=[6, 7],\n",
    "                        hl_label='dev', equal_scale=True)\n",
    "\n",
    "# setup the colors for labels\n",
    "c=cm.Accent\n",
    "colors=[]\n",
    "color_labels = np.arange(n_clusters)\n",
    "for i in range(n_clusters):\n",
    "    colors.append(c.colors[i])\n",
    "tag_cmap = ListedColormap(colors)\n",
    "loc = np.linspace(0, 1, n_clusters+1)\n",
    "loc = loc + 1/(2*n_clusters)\n",
    "\n",
    "for n, ax in enumerate(fig.axes[:-1]):\n",
    "    ax.pcolormesh(ts[n], np.array([0,0.5]), labels[n][np.newaxis, :len(ts[n])-1], shading='flat', cmap=tag_cmap,\n",
    "                 vmin=0, vmax=n_clusters)\n",
    "    ax.axhline(0.5, 0, 1, color='k', linewidth=1)\n",
    "\n",
    "fig.subplots_adjust(right=0.93)\n",
    "cbar_ax = fig.add_axes([0.95, 0.2/3, 0.01, 0.4])\n",
    "cb = fig.colorbar(cm.ScalarMappable(norm=matplotlib.colors.Normalize(vmin=0,vmax=1.), cmap=tag_cmap), cax=cbar_ax)\n",
    "cb.set_ticks(loc)\n",
    "cb.set_ticklabels(color_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70511f9-2258-4eda-a25f-5b3e6d6b003b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Numpy vs sklearn\n",
    "\n",
    "We could also perform PCA directly with `numpy` either with the eigendecomposition of the covariance matrix of the data or with an SVD decomposition.\n",
    "Note that the components can differ slightly since by default `sklearn` does approximations depending on the size of the input data. Also note that it's possible that some of the axes get reversed (i.e. multiplied by -1).\n",
    "\n",
    "In the case of SVD the correspondence is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a1ec4d-89f6-4c94-bb7c-1723a8980a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, t, spec = signal.spectrogram(samples[0], sample_rate)\n",
    "X = pcc.split_spec(StandardScaler().fit_transform(spec.T).T, wdw, stride=stride)\n",
    "_, pca, _ = pcc.spec2pca([spec], wdw, stride=stride, n_components=5)\n",
    "\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# S contains the singular values and the columns of U are the (scaled) principal components\n",
    "U, S, V_T = np.linalg.svd(X, full_matrices=False)\n",
    "V = V_T.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c9807a-fb1c-4263-907e-54794f996199",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.singular_values_, S[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8037bf2-a347-4cc9-a847-0f0ed08c3013",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_[:,:5], V_T[:5, :5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
