{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4d07a6a-44c1-4654-9fd7-f702020f045c",
   "metadata": {},
   "source": [
    "*Make sure to run the whole notebook first from the `Runtime -> Run all` menu item.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975190a1-e287-46a1-b392-ba519151ae11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "from math import ceil\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "import ipywidgets as widgets\n",
    "from scipy.spatial.distance import pdist, squareform, cdist\n",
    "!wget 'https://github.com/solita/ivves-machine-spraak/blob/main/courses/methods/audio/jazz.wav?raw=true' -O 'jazz.wav';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a8d071-36b5-42fb-90ec-a3749cd6e776",
   "metadata": {},
   "source": [
    "# Methods of Audio Analysis in Python - Lesson 3\n",
    "# Self-similarity, Kernels and Structural Analysis\n",
    "\n",
    "In this lesson we'll introduce another method to extract structure from audio data (or any ordered sequence for that matter), but this time in the temporal direction (recall that for the cepstrum we were interested in the frequency-axis). We do this by comparing the signal at each timestep to the past (and future) values in order to obtain a measure of its **self-similarity**. This is not much different from computing the autocorrelation of some sequence, except that we are afforded more freedom in deciding what *similarity* actually means in the given context. The power of this method, however, comes from the way we choose to *represent* this information. We organise the similarity values into a matrix (the **self-similarity matrix** (**SSM**)), which allows us to compare different segments across time over the whole input clip. This not only permits the use of analytical methods to detect structure in the signal, but also lets us easily *visualise* this information. The importance of effective visualisation tools should not be underestimated for explorative analysis. The final advantage of the self-similarity method is that it is extremely versatile: it is not in any way reliant on spectral decomposition (even though that's where we'll apply it in this notebook), but works for many different types of signals and feature representations. It can also be used to creatively solve many types of problems of which we'll give a few simple examples in today's lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285c7341-b566-47c6-9723-2afac1e2697b",
   "metadata": {},
   "source": [
    "## Self-similarity matrix\n",
    "\n",
    "We will use the following 30 second sample of a jazz song as a running example. Let's load it up and give it a listen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ef9b02-373f-43d2-9a25-ef10f810df23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original recording courtesy of Tri-Tachyon @ freesound.org, converted to WAV.\n",
    "# https://freesound.org/people/Tri-Tachyon/sounds/466478/\n",
    "# Licensed under the Creative Commons Attribution 4.0 license\n",
    "# https://creativecommons.org/licenses/by/4.0/\n",
    "sr, jazz = wavfile.read('jazz.wav')\n",
    "# Normalise\n",
    "jazz = jazz / np.abs(jazz).max()\n",
    "# The warning below appears, because the wav file contains additional metadata\n",
    "print(f'Successfully read audio with sampling rate {sr} Hz, {jazz.shape[1]} channels and length {jazz.shape[0] / sr:.1f} s.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ec76b9-5e80-4149-828c-d5117a9aefe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that we need the transpose since IPython and SciPy use the opposite convention for the array shape\n",
    "Audio(jazz.T, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238d444a-f60b-4881-b629-022e06d1bf93",
   "metadata": {},
   "source": [
    "We can hear that in this relaxed piece (which is in $4\\atop 4$ signature) there's a 4 bar sequence (call it **A**) that repeats twice before the solo guitar (and the finger snapping!) kicks in for another 8 bars. The drum beat also changes slightly for this latter part, which we denote by **B**. So the structure of this song is **AAB**. Our aim is to be able to detect this algorithmically, but first let's see what we can notice from the spectrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30aed387-2794-4c2f-ad90-c371492c7dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use some sensible values for the FFT parameters. It could be useful to tweak these\n",
    "# depending on what you want to achieve.\n",
    "window_len = 1024\n",
    "window_overlap = window_len // 4\n",
    "mode = 'magnitude'\n",
    "f, t, Sxx = signal.spectrogram(jazz[:, 0], sr, nperseg=window_len, noverlap=window_overlap, mode=mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942684c3-0940-42a2-993a-be6032299b23",
   "metadata": {},
   "source": [
    "We'll plot the spectrogram on the decibel scale. You can again use the slider to zoom in on the lower frequencies.\n",
    "\n",
    "> ðŸ”Š Exercise: Try out different FFT parameters and computing the power spectrum instead of the magnitude (mode='psd'), for example. How does this affects the resulting visualisation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf232ae-5dfc-4255-9cbe-56ebc9684a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As mentioned in lecture 1, it is usually helpful to convert to\n",
    "# the logarithmic decibel scale.\n",
    "Sxx_dB = 10*np.log10(Sxx / np.max(Sxx))\n",
    "\n",
    "def plot(y_UB):\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    p = ax.pcolormesh(t, f, Sxx_dB, cmap='magma', shading='auto',\n",
    "    # For other types of spectrograms you might have to adjust the vmin\n",
    "    # and vmax values, which control the colormap scaling, or just leave\n",
    "    # them out completely (i.e. scale automatically)\n",
    "                      vmin=-50, vmax=0)\n",
    "    ax.set_ylim(0, y_UB)\n",
    "    ax.set_xlabel('Time (s)')\n",
    "    ax.set_ylabel('Frequency (Hz)')\n",
    "    ax.set_title('Spectrogram of jazz song')\n",
    "    fig.colorbar(p, label='dBFS')\n",
    "    plt.show()\n",
    "\n",
    "widgets.interact(plot, y_UB=widgets.IntSlider(value=22000, min=500, max=22000,\n",
    "                                             description='Max frequency'));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d0e210-6e5a-4133-b63c-db5b0a91ae72",
   "metadata": {},
   "source": [
    "It's quite easy to notice some beat structure in thhe spectrogram, which shows up as brighter vertical lines (when the signal's energy increases across the spectrum). We'll return to this later, but let's compute the self-similarity matrix first. To do this we need to decide on a *similarity metric*. We choose to use the standard Euclidean distance, while other common choices include the scalar product (or the normalised cosine distance), Minkowski distance or Manhattan distance. For more examples see [SciPy documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.pdist.html).\n",
    "\n",
    "Denote the similarity metric by $d$ and our feature sequence (i.e. the sequence of vectors in the frequency-domain that we obtain from the FFT) by $\\langle X_{n}\\rangle$. The self-similarity matrix $S$ is then given by\n",
    "$$S=(s_{ij})=d(X_{i}, X_{j}).$$\n",
    "We will always follow the convention that $d(u, u)=0$ so that equal vectors have similarity score 0 and that $d(u, v)\\geq 0$. The similarity score can either be bounded above (as it is, for example, for the cosine distance) or unbounded (like for the Euclidean distance). Be aware though that this is not a universal convention and it varies in the literature. Nevertheless, we can visualise $S$ as a heatmap so that the $s_{00}$ entry of $S$ also correspond to the upper left-hand corner of the visualisation (again, you will find other conventions in the literature). This computation is very easy to do with the help of SciPy's `scipy.spatial.distance.pdist()` and `scipy.spatial.distance.squareform()`. You should refer to the documentation ([here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.pdist.html) and [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.squareform.html)) to make sure you understand what is happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef69d71-43bb-419f-971e-0de62f2b067b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "metric = 'euclidean'\n",
    "# Note that we have to pass the transpose of Sxx to match with\n",
    "# the convention of pdist\n",
    "S = squareform(pdist(Sxx.T, metric=metric))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "p = ax.imshow(S, cmap='hot', interpolation='nearest', aspect='equal')\n",
    "ax.set_title('SSM for jazz song')\n",
    "fig.colorbar(p, shrink=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28220b4c-b0f6-49a1-9d01-2df21404ee16",
   "metadata": {},
   "source": [
    "In the SSM plot the $x$- and $y$-axes both correspond to the index of the (frequency) vector sequence, i.e. `Sxx`. In other words, the $(i, j)$ element corresponds to the similarity of `Sxx[:, i]` and `Sxx[:, j]`.\n",
    "The way to read the plot then is to start from the upper left-hand corner (the (0, 0)-cell) and move along the main diagonal. As we would expect, the main diagonal itself is completely black (since $d(a, a)=0$)...\n",
    "\n",
    "> ðŸ”Š Exercise: Continue with your FFT parameter experimentation and check also how it affects the SSM. Try out other similarity metrics as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60e6936-8f1a-4a41-8ce9-ec8d3c38b637",
   "metadata": {},
   "source": [
    "## Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1ee9ba-c2e6-41c0-a610-ab415066bbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel(size=16, smoothing='', strength=2**2, kernel_primitive=None):\n",
    "    \"\"\"Computes a kernel to be used for feature extraction from similarity\n",
    "    matrices. This is done based on the given `kernel_primitive` with the\n",
    "    default argument corresponding to a checkerboard kernel, as explained\n",
    "    in Segmentation.ipynb.\n",
    "\n",
    "    Arguments:\n",
    "        size\n",
    "            - Desired size of the kernel (as a square matrix). Note that `size`\n",
    "                must be divisible by `len(kernel_primitive)` (default: 16)\n",
    "\n",
    "        smoothing\n",
    "            - The default argument corresponds to no smoothing, while the value\n",
    "                'gaussian' applies a Gaussian smoothing with bandwidth given by\n",
    "                `strength` (default: '')\n",
    "\n",
    "        strength\n",
    "            - Bandwidth for Gaussian smoothing (default: 4)\n",
    "\n",
    "        kernel_primitive\n",
    "            - Kernel primitive which is enlargened to K. The default argument\n",
    "                corresponds to checkerboard kernel (default: none)\n",
    "\n",
    "    Returns:\n",
    "        K\n",
    "            - A `size` x `size` numpy array\n",
    "    \"\"\"\n",
    "    # strength is inversely proportional to the standard deviation of the\n",
    "    # smoothing and thus strength -> 0 corresponds to reduced smoothing,\n",
    "    # whereas strength -> infty completely dampens the input signal (except\n",
    "    # at the mean)\n",
    "    if kernel_primitive is None:\n",
    "        kernel_primitive = np.array([[-1, 1],\n",
    "                                    [1, -1]])\n",
    "    assert size % len(kernel_primitive) == 0, f'Size {size} should be divisible by the primitive length {len(kernel_primitive)}'\n",
    "    kernel_scale = size // len(kernel_primitive)\n",
    "    K_raw = np.kron(kernel_primitive, np.ones((kernel_scale, kernel_scale)))\n",
    "    assert K_raw.shape[0] == K_raw.shape[1]\n",
    "    if not smoothing:\n",
    "        K = K_raw\n",
    "    elif smoothing == 'gaussian':\n",
    "        # create a grid of X & Y values based on which\n",
    "        # we compute the Gaussian smoothing\n",
    "        X = Y = np.arange(0, len(K_raw))\n",
    "        XX, YY = np.meshgrid(X, Y)\n",
    "        mu = len(K_raw) // 2\n",
    "        bw = size / strength\n",
    "        gauss = np.exp(-.5 * ((XX-mu) ** 2 + (YY-mu) ** 2) / bw ** 2)\n",
    "        smoothing = gauss * np.ones((len(K_raw), len(K_raw)))\n",
    "        K = K_raw * smoothing\n",
    "    else:\n",
    "        raise Exception(\"Unknown smoothing specifier.\")\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28d9e79-30e4-4e52-a88b-f18449f18e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = kernel(32, 'gaussian', strength=4)\n",
    "fig, ax = plt.subplots(figsize=(4,4))\n",
    "ax.imshow(K, cmap='RdBu')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73ca0d6-0e4d-4678-ad36-6b2ada6b66f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "S_nov = signal.convolve2d(S, K / (len(K) ** 2), mode='same')\n",
    "nov = np.diag(S_nov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2286db6-4d85-4aa4-83c8-00e39d6b28e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(figsize=(8, 10), nrows=2, gridspec_kw={'height_ratios': [1, 4]})\n",
    "ax1.plot(t, nov)\n",
    "ax1.set_xmargin(0)\n",
    "ax2.imshow(S, cmap='hot', interpolation='nearest', aspect='equal')\n",
    "ax2.axis(False)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d71d75a-e263-4e03-981c-0ae3a02630c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "peaks = signal.find_peaks(nov, height=0.003, prominence = 0.003)[0]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 3))\n",
    "ax.plot(t, nov)\n",
    "for p in peaks:\n",
    "    ax.axvline(t[p], color='lime', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bba5dc-9b85-4303-a298-9e0269ccd479",
   "metadata": {},
   "source": [
    "## BPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cd908a-015d-4ee8-8033-e13879f655ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "metric = 'euclidean'\n",
    "S_dB = squareform(pdist(Sxx_dB.T, metric=metric))\n",
    "S_dB_nov = signal.convolve2d(S_dB, K / (len(K) ** 2), mode='same')\n",
    "nov_dB = np.diag(S_dB_nov)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.imshow(S_dB, cmap='hot', interpolation='nearest', aspect='equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8837bc02-959b-483f-b73b-6a2215e7fc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "peaks_dB = signal.find_peaks(nov_dB, height=10, prominence=10, distance=50)[0]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 3))\n",
    "ax.plot(t, nov_dB)\n",
    "ax.set_ylim(-1, 25)\n",
    "for p in peaks_dB:\n",
    "    ax.axvline(t[p], color='lime', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e471e35a-a57b-4cdd-8496-0d2ef082f3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "beats = np.array([t[p] for p in peaks_dB])\n",
    "gaps = np.diff(beats)\n",
    "mean_gap = np.mean(gaps)\n",
    "print(f'Detected BPM is {60 / mean_gap:.0f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7298d2-29ae-4372-909b-037be5e65464",
   "metadata": {},
   "source": [
    "## Where to next?\n",
    "\n",
    "## Summary\n",
    "\n",
    "## Further reading\n",
    "\n",
    "<http://www.musanim.com/wavalign/foote.pdf>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a63ecf-f436-4d8a-b410-dffa24cc4ad2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
