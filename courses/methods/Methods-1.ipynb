{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c30991c-6250-429b-a0f3-e5d4636c2f43",
   "metadata": {},
   "source": [
    "*Make sure to run the whole notebook before beginning from the `Runtime -> Run all` menu item.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e13d7c-813c-47b5-9bb4-41665f45dd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import signal\n",
    "from math import ceil\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51d9436-e92a-4078-9263-cb330ab3a547",
   "metadata": {},
   "source": [
    "# Methods of Audio Analysis in Python - Lesson 1\n",
    "# Digital Signal Processing and Spectral Analysis\n",
    "\n",
    "Welcome to the first lesson of the [Methods of Audio Analysis in Python](https://github.com/solita/ivves-machine-spraak/tree/main/courses/methods) mini-course.\n",
    "In this lesson we'll learn how an analogue signal is transformed into a digital format via a sampling procedure. Then we observe that audio signals quickly become\n",
    "extremely complicated and tricky to analyse directly. To remedy this we separate the contribution of oscillations of different frequencies to the full signal.\n",
    "This is done via a mathematical method known as the Fourier transform. Finally, we'll see how we can obtain a very informative representation of our sound data\n",
    "by repeatedly applying this decomposition to short segments of the input signals by performing a windowing operation. Let's jump straight to it, good luck!\n",
    "\n",
    "### Sampling rate\n",
    "\n",
    "Sound is produced by physical vibrations that cause fluctuations in air pressure (or another medium) around them. The frequencies of this vibration determine the pitch of the sound that is also transmitted into your ears. The pressure differentials reaching your eardrums cause them to vibrate at the same frequencies, which your brain interprets as sound. The vibration of the membrane of your eardrum is governed by the equation (up to scaling by volume)\n",
    "\n",
    "$$y = \\sin(2\\pi f t),$$\n",
    "\n",
    "where $f$ is the frequency and $t$ is time. We can visualise this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7fc42f-c609-4484-ac65-0ce4afb9755e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "freq = 440\n",
    "x = np.linspace(0, 2/freq, num=800)\n",
    "A_4 = np.sin(2 * np.pi * freq * x)\n",
    "plt.figure(figsize=(12, 3))\n",
    "plt.plot(x, A_4)\n",
    "plt.xlabel('time (s)')\n",
    "plt.ylabel('amplitude')\n",
    "plt.axhline(0, c='k', lw=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3351ee-d396-4832-8189-d64c73982963",
   "metadata": {
    "tags": []
   },
   "source": [
    "Here the amplitude of the oscillation depends on the volume of the sound. The above graph corresponds to a very short segment of the note A<sub>4</sub> (corresponding to the frequency 440 Hz). We can also listen to it with Python with the help of `IPython.display.Audio()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a855bc7-8613-4706-ad1f-f7e4ac29c759",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = 440\n",
    "duration = 2\n",
    "rate = 48000\n",
    "\n",
    "# We use numpy's linspace() function to create an array of equally spaced values between 0 and `duration`\n",
    "# The number of points to be generated is given by the num-argument.\n",
    "t = np.linspace(0, duration, num=duration * rate)\n",
    "y = np.sin(2 * np.pi * freq * t)\n",
    "\n",
    "# The Audio() object of the IPython module is useful when working with audio analysis\n",
    "# in Python. Its arguments are a list or ndarray (numpy array) of samples of the audio signal\n",
    "# and the sample rate of the recording. Note that VSCode does not currently support audio playback.\n",
    "Audio(y, rate=rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7549bb28-3957-4727-917e-2a54456cdda8",
   "metadata": {},
   "source": [
    "ðŸ”Š **Exercise:** Try adjusting the sample rate for the 440 Hz sine wave in the above cell. Does it affect what you hear? What happens if you set the sample rate very low? Why do you think that is?\n",
    "\n",
    "In the above, the `rate` parameter is known as the **sampling rate**, which is a necessary evil when we are trying to represent a continuous signal, such as sound, in digital form. What happens is that in order to store the signal we take samples of it at (typically) equally spaced intervals and record the corresponding sequence of amplitudes in a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b00206-5e4f-40c2-a486-1aa31b2dd54e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from math import ceil, floor\n",
    "freq = 440\n",
    "periods = 2\n",
    "x = np.linspace(0, periods/freq, num=800)\n",
    "y = np.sin(2 * np.pi * freq * x)\n",
    "plt.figure(figsize=(12, 3))\n",
    "samples = 7\n",
    "plt.plot(x, y)\n",
    "idx = np.linspace(0, len(x)-1, num=samples, endpoint=True, dtype=int)\n",
    "plt.scatter(x[idx], y[idx], s=42, c='r', zorder=10)\n",
    "plt.vlines(x[idx], 0, y[idx], ls='--', color='grey')\n",
    "plt.xlabel('time (s)')\n",
    "plt.ylabel('amplitude')\n",
    "plt.axhline(0, c='k', lw=1)\n",
    "plt.show()\n",
    "print(f'samples = {np.array2string(y[idx], precision=2, separator=\", \", sign=\" \")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368ec983-389c-40d4-b793-4b58989e2fd9",
   "metadata": {},
   "source": [
    "The sampling rate is then the number of samples we record per second. Then, in order to play back the digitised signal your computer attempts to reconstruct the original waveform based on the samples. Obviously, the more samples we take, the more accurate is the representation at the cost of consuming more memory. Below we visualise a naive linear interpolation scheme for reconstructing the original audio signal. Use the slider to experiment how the sampling rate affects the approximation.\n",
    "\n",
    "> *In reality a more sophisticated interpolation scheme could be used, but that's out of scope for these lectures.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aacd61d-4c9b-4fd8-9128-a3724572279a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "freq = 440\n",
    "periods = 2\n",
    "x = np.linspace(0, periods/freq, num=800)\n",
    "y = np.sin(2 * np.pi * freq * x)\n",
    "def plot(n_samples):\n",
    "    plt.figure(figsize=(12, 3))\n",
    "    plt.plot(x, y, alpha=0.5)\n",
    "    idx = np.linspace(0, len(x)-1, num=n_samples, endpoint=True, dtype=int)\n",
    "    plt.plot(x[idx], y[idx], c='k')\n",
    "    plt.scatter(x[idx], y[idx], s=42, c='r', zorder=10)\n",
    "    plt.vlines(x[idx], 0, y[idx], ls='--', color='grey')\n",
    "    plt.xlabel('time (s)')\n",
    "    plt.ylabel('amplitude')\n",
    "    plt.axhline(0, c='k', lw=1)\n",
    "    plt.show()\n",
    "\n",
    "widgets.interact(plot, n_samples=widgets.IntSlider(value=7, min=2, max=30));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d94f0e7-2a33-4ed9-bca2-1db332adf128",
   "metadata": {},
   "source": [
    "### Nyquist frequency\n",
    "\n",
    "In real life, sound is rarely so well-behaved. Even for simple signals the above exercise illustrates that it is necessary to store a lot of data if we want to have a faithful representation of the sound we are trying to analyse. In reality, the sounds we hear often comprise of a multitude of frequencies all mixed together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564c3314-346a-463e-8068-a0d8465f3b5e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "C_freqs = np.array([[261.626, 329.628, 391.995]])\n",
    "x = np.linspace(0, 4/C_freqs[0][0], num=800)\n",
    "C_major = np.sum(np.sin(2 * np.pi * C_freqs.T * x), axis=0) / C_freqs.shape[1]\n",
    "plt.figure(figsize=(12, 3))\n",
    "plt.plot(x, C_major)\n",
    "plt.xlabel('time (s)')\n",
    "plt.ylabel('amplitude')\n",
    "plt.axhline(0, c='k', lw=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ea19c0-9359-4d84-b6da-d6f8d00320fe",
   "metadata": {},
   "source": [
    "The more complex the signal gets, the more samples we need to maintain the same level of accuracy for the reconstructed signal. In the above graph we've visualised the waveform for the C-major chord (which consists of three notes). You can listen to it below:\n",
    "(add formula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bdba8a-a0c5-4f74-adc4-493436a92c97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "freqs = np.array([[261.626, 329.628, 391.995]])\n",
    "duration = 2\n",
    "rate = 48000\n",
    "t = np.linspace(0, 2, num=duration * rate)\n",
    "y = np.sum(np.sin(2 * np.pi * freqs.T * t), axis=0) / freqs.shape[1]\n",
    "Audio(y, rate=rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6d7123-71fb-4a0f-94bb-a316c22f9500",
   "metadata": {},
   "source": [
    "ðŸ”Š **Exercise:** Try adjusting the sample rate for the C chord. What do you notice compared to the earlier example of the A<sub>4</sub> note?\n",
    "\n",
    "You might notice that there is a huge discrepancy in the information content of the input signal versus what we would store on disk. To create the sound, all we did was specify three pieces of information (the frequencies), but we find that to store the data we need thousands of sample points per second! It turns out that relying on amplitude based representation is *fast* and *easy*, but also very wasteful. So why not just reconstruct the audio based on the frequencies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8203d8e8-e2e0-4aff-81fa-f8115d6214c4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "C_freqs = np.array([[261.626, 329.628, 391.995]])\n",
    "x = np.linspace(0, 4/C_freqs[0][0], num=800)\n",
    "C_major = np.sum(np.sin(2 * np.pi * C_freqs.T * x), axis=0) / C_freqs.shape[1]\n",
    "y = C_major\n",
    "def plot(samples):\n",
    "    idx = np.linspace(0, len(x)-1, num=samples, endpoint=True, dtype=int)\n",
    "    plt.figure(figsize=(12, 3))\n",
    "    plt.plot(x[idx], y[idx], c='k')\n",
    "    plt.scatter(x[idx], y[idx], s=42, c='r', zorder=10)\n",
    "    plt.vlines(x[idx], 0, y[idx], ls='--', color='grey')\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel('time (s)')\n",
    "    plt.ylabel('amplitude')\n",
    "    plt.axhline(0, c='k', lw=1)\n",
    "    plt.show()\n",
    "\n",
    "widgets.interact(plot, samples=widgets.IntSlider(value=14, min=2, max=30));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112ff4ec-90d2-4830-8af3-6f700e1db343",
   "metadata": {},
   "source": [
    "On the other hand, if we get unlucky, two different waves could produce the exact same quantisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6287a3-f442-4d33-8886-3419b580487c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "freq = 440\n",
    "\n",
    "def plot(n_samples, periods):\n",
    "    # These are the multipliers to get aliased waves.\n",
    "    # Note that m1, m2 == 1 (mod n_samples-1) and that\n",
    "    # m1 & m2 don't depend on `periods` or `freq`\n",
    "    m1 = 1 + (n_samples-1) * 1\n",
    "    m2 = 1 + (n_samples-1) * 2\n",
    "    \n",
    "    x = np.linspace(0, periods/freq, num=10000)\n",
    "    y1 = np.sin(2 * np.pi * freq * x)\n",
    "    y2 = np.sin(2 * np.pi * (m1*freq) * x)\n",
    "    y3 = np.sin(2 * np.pi * (m2*freq) * x)\n",
    "    idx = np.linspace(0, len(x)-1, num=n_samples, endpoint=True, dtype=int)\n",
    "\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(nrows=3, ncols=1, figsize=(12, 7), sharex=True)\n",
    "    ax1.plot(x, y1, alpha=0.5, label=f'{freq} Hz')\n",
    "\n",
    "    ax1.plot(x[idx], y1[idx], c='k')\n",
    "    ax1.scatter(x[idx], y1[idx], s=42, c='r', zorder=10)\n",
    "    ax1.vlines(x[idx], 0, y1[idx], ls='--', color='grey')\n",
    "    ax1.legend(loc='upper right')\n",
    "\n",
    "    ax2.plot(x, y2, alpha=0.5, label=f'{m1*freq} Hz')\n",
    "    ax2.plot(x[idx], y2[idx], c='k')\n",
    "    ax2.scatter(x[idx], y2[idx], s=42, c='r', zorder=10)\n",
    "    ax2.vlines(x[idx], 0, y2[idx], ls='--', color='grey')\n",
    "    ax2.legend(loc='upper right')\n",
    "\n",
    "    ax3.plot(x, y3, alpha=0.5, label=f'{m2*freq} Hz')\n",
    "    ax3.plot(x[idx], y3[idx], c='k')\n",
    "    ax3.scatter(x[idx], y3[idx], s=42, c='r', zorder=10)\n",
    "    ax3.vlines(x[idx], 0, y3[idx], ls='--', color='grey')\n",
    "    ax3.legend(loc='upper right')\n",
    "\n",
    "    ax3.set_xlabel('time (s)')\n",
    "    for ax in [ax1, ax2, ax3]:\n",
    "        ax.set_ylabel('amplitude')\n",
    "        ax.axhline(0, c='k', lw=1)\n",
    "        #ax.set_xlim(0, 0.0035)\n",
    "    plt.show()\n",
    "    \n",
    "widgets.interact(plot,\n",
    "                 n_samples=widgets.IntSlider(value=7, min=2, max=20),\n",
    "                 periods=widgets.IntSlider(value=2, min=1, max=5));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438eb22d-6d63-4d4c-890e-d4550b36c280",
   "metadata": {},
   "source": [
    "> â˜•ï¸ **Aside (HARD):** How did we come up with the frequencies above? Note that 3080 / 440 = 7, 5720 / 440 = 13.\n",
    "\n",
    "> *Hint #1: These multiples are independent of the base frequency (you can check by altering the code above).*\n",
    "\n",
    "> *Hint #2: You have to consider the divisibility of the multipliers w.r.t. the number of samples.*\n",
    "You can check the answer from the hidden code cell above.\n",
    "\n",
    "This phenomenon, when two distinct signals produce the same sampled values, is called **aliasing**.\n",
    "So how can we ensure that we avoid such situations? That is, how high should we set our sampling rate?\n",
    "\n",
    "It turns out that the *sampling rate should be at least twice the highest frequency component* of the input signal.\n",
    "That is, if we let $f_{s}$ denote the sampling frequency and $f_{max}$ the highest frequency component present in the input signal, then\n",
    "we need\n",
    "\n",
    "$$ f_{s} \\geq 2 f_{max}.$$\n",
    "\n",
    "This is called the **Nyquist criterion** and the minimal required sampling frequency (i.e. $2 f_{max}$) is called the **Nyquist rate**.\n",
    "\n",
    "The main takeaway here is that if you wish to record audio for analysis purposes then in order to ensure a truthful representation\n",
    "of the original signal you should keep the sampling rate above twice the highest frequency component that you might expect.\n",
    "In practice common choices are e.g. 48 kHz, 96 kHz or 192 kHz (see [Sampling (Wikipedia)](https://en.wikipedia.org/wiki/Sampling_(signal_processing)#Audio_sampling)). In constrained situations you might have to survive with a lower resolution, but then it could be worth investigating how the lower sampling rate impacts the robustness of the analysis in that particular setting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6b49d7-b26f-444a-b94e-467de1403040",
   "metadata": {},
   "source": [
    "### Fourier Transform\n",
    "\n",
    "We now know how the sampling rate affects the signal and how it is related to the frequency content resolution of the digitised signal, but how can we figure out the frequency content in the first place? Even trying to determine that the wave in the above example of the C-major chord is a sum of three different waves of distinct frequencies seems hopeless based just on the plot.\n",
    "\n",
    "The answer to this problem is provided by the **Fourier transform**, which is a mathematical method for figuring out the periodic components of a function (by essentially comparing the function with waves of all possible frequencies). The matter is complicated slightly by the fact that we are dealing with a *sequence of samples* of a function rather than the actual continuous function itself. This means that we have to use an approximation called the **discrete Fourier transform** (DFT), which is usually implemented via a very efficient algorithm known as the **Fast Fourier Transform** (FFT). We won't go into the theoretical details in these lectures, but we can still understand what happens in practice.\n",
    "\n",
    "So, suppose we have our input sequence, which in this instance is a set of samples of an audio waveform, and denote it by $\\langle x_{n}\\rangle = \\{x_{0}, x_{1}, \\ldots, x_{N-1}\\}$ (i.e. $x_{0}$ is the first red dot above and we have $N$ sample points in our sequence in total). The FFT then transforms this sequence into a new sequence $\\langle X_{k} \\rangle = \\{X_{0}, X_{1}, \\ldots, X_{N-1}\\}$ defined by\n",
    "$$X_{k} = \\sum_{n=0}^{N-1} x_{n}\\left[\\cos\\bigl(\\tfrac{2\\pi}{N}kn\\bigr)-i\\sin\\bigl(\\tfrac{2\\pi}{N}kn\\bigr)\\right].$$\n",
    "\n",
    "The actual formula is not too important here, but what we care about is that $X_{k}$ *picks out the $k/N$ frequency component of $\\langle x_{n}\\rangle$*. In other words $X_{k}$ tells us how much (i.e. the *magnitude*) of the $k/N$ frequency is approximately present in the input signal. Actually $X_{k}$ is a complex number so in addition to magnitude it also contains phase information, but usually we are not too interested in this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479766ea-00a1-49bf-b1b4-3f322d6d6d4d",
   "metadata": {},
   "source": [
    "Enough of the theory, let's try it out in practice. In `numpy`, methods related to the DFT are implemented in the module `np.fft`.\n",
    "In order to compute the FFT, we use the function `np.fft.rfft()`. The `r` in the function name signifies that we are computing the transformation of a *real-valued* sequence. This is always the case when the input is audio data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb9f79e-afb8-4115-a3c8-8aaf56fdf678",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot(sample_rate, duration):\n",
    "    # We use the same sample data of a C major chord as in the above examples,\n",
    "    # except observed for a longer duration. This is so that the DFT\n",
    "    # approximation gets more accurate.\n",
    "    x = np.linspace(0, duration, num=int(duration * sample_rate))\n",
    "    y = np.sum(np.sin(2 * np.pi * C_freqs.T * x), axis=0) / C_freqs.shape[1]\n",
    "\n",
    "    # We pass the sample sequence to np.fft.rfft(), it returns the transformed\n",
    "    # sequence of complex numbers describing the frequency content of y.\n",
    "    # We take absolute values in order to obtain the magnitude of each frequency.\n",
    "    y_abs = np.abs(np.fft.rfft(y))\n",
    "\n",
    "    # np.fft.rfftfreq() is a helper function that tells us what are the actual frequencies\n",
    "    # that the magnitudes stored in y_abs correspond to.\n",
    "    freq = np.fft.rfftfreq(x.size, d=1./sample_rate)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(14, 4), gridspec_kw={'width_ratios': [1, 2]})\n",
    "    ax1.plot(x, y)\n",
    "    ax1.set_xlabel('time (s)')\n",
    "    ax1.set_ylabel('amplitude')\n",
    "    ax1.set_ylim(-1., 1.)\n",
    "    \n",
    "    ax2.plot(freq, y_abs)\n",
    "    for i, f in enumerate(C_freqs.ravel()):\n",
    "        ax2.axvline(f, c='C6', ls='--', label='ground truth' if i == 0 else None)\n",
    "    ax2.axvline(sample_rate/2, c='C1', label='Nyquist freq')\n",
    "    ax2.axvline(2 * np.max(C_freqs), c='C2', label='Nyquist rate')\n",
    "    ax2.scatter(sample_rate, -5, s=42, c='r', label='sampling rate', clip_on=False)\n",
    "    #s.set_clip_on(False)\n",
    "    ax2.set_ylim(-5, 200)\n",
    "    ax2.set_xlim(0, 1050)\n",
    "    ax2.set_xlabel('frequency (Hz)')\n",
    "    ax2.legend(loc='upper right')\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "widgets.interact(plot,\n",
    "                 sample_rate=widgets.IntSlider(value=900, min=50, max=1000),\n",
    "                 duration=widgets.FloatSlider(value=1, min=0.1, max=2, step=0.1));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f145147d-3393-4ad2-b2e8-bac4e2e5dd43",
   "metadata": {},
   "source": [
    "Above we plot the input signal on the left and the magnitude of the DFT on the right (with blue). The pink vertical lines signify the true frequencies of C major, which are what we used to generate the sample data.\n",
    "\n",
    "> ðŸ”Š **Exercise**: Experiment with adjusting the sampling rate and duration of the input signal and notice how it affects the accuracy of the DFT.\n",
    "\n",
    "We also visualise the sampling rate and the corresponding Nyquist frequncy as well as the Nyquist rate corresponding to this input date. You can see how lowering the sampling rate impacts our ability to detect the higher frequencies.\n",
    "\n",
    "It's crucial to remember that we are performing implicit approximations at many steps of the process in going from the original audio signal to the sampled sequence and eventually to the resulting DFT. Hence the results *will not be exact*, as we can see above, and information leaks into nearby frequency bins."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c3c413-a692-4a7a-a663-f846d20c1e1b",
   "metadata": {},
   "source": [
    "---\n",
    "### Windowing\n",
    "\n",
    "Given an audio signal, we now know how to get a summary of its frequency content over the whole duration. This is already highly useful and many problems can be solved just by computing the DFT and looking at the distribution of the resulting frequencies in order to e.g. classify different audio clips.\n",
    "However, this is a coarse summary statistic of the input audio and as the length of the clip increases its usefulness goes down (or rather the information content does -- think about computing averages of features over a large data set). In order to overcome this problem we apply a very simple trick: **windowing**.\n",
    "\n",
    "In essence, all we do is split the input signal into pieces (which can be overlapping or disjoint) and compute the DFT of each piece separately. In practice, we obtain better results if we weigh the sample points towards the edges of each clip a bit less than the ones in the middle. This weighting is described by a **window function**.\n",
    "\n",
    "There's a whole assortment of window functions available via SciPy's signal processing module `scipy.signal`, see the [documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.get_window.html) for a full list. Let's visualise a few of these below.\n",
    "We use `scipy.signal.get_window()` to get the window data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa63a4e3-689b-471d-8328-da4ac09e2c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "windows = ['boxcar', 'hamming', 'hann', 'tukey']\n",
    "\n",
    "ncols = len(windows)\n",
    "fig, axs = plt.subplots(ncols=ncols, figsize=(ncols*4, 4))\n",
    "for ax, wdw in zip(axs.flat, windows):\n",
    "    N = 100\n",
    "    x = np.linspace(0, 1, N)\n",
    "    w = signal.get_window(wdw, N)\n",
    "    ax.plot(x, w)\n",
    "    ax.set_title(wdw)\n",
    "    ax.set_ylim(0, 1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c4447d-0753-4f19-8e5c-cce08dfe11ef",
   "metadata": {},
   "source": [
    "Above on the left is the naive flat window, where each sample is weighted equally by 1. The following Hamming, Hann and Tukey windows are common choices and have been observed to perform well in many domains. We'll see another option in the next section. In general, there is no universal answer for which window type is best and you have to experiment with it yourself. On the other hand, the standard choices are usually OK and worrying about it further is unlikely to have much impact on your analysis.\n",
    "\n",
    "Now, in order to perform the actual windowing we also need to determine how many sample points to include in each window. This is called the **window length**, which, together with **window overlap**, tells us what we need to split the signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd9f7db-ea08-4175-a19d-d60ccbe6d60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "C_freqs = np.array([[261.626, 329.628, 391.995]])\n",
    "x = np.linspace(0, 4/C_freqs[0][0], num=800)\n",
    "C_major = np.sum(np.sin(2 * np.pi * C_freqs.T * x), axis=0) / C_freqs.shape[1]\n",
    "y = C_major\n",
    "n_samples = 75\n",
    "\n",
    "window = 'hamming'\n",
    "window_len = 9\n",
    "window_overlap = 3\n",
    "\n",
    "def plot(window, window_len, window_overlap):\n",
    "    idx = np.linspace(0, len(x)-1, num=n_samples, endpoint=True, dtype=int)\n",
    "    sample_x = x[idx]\n",
    "    sample_y = y[idx]\n",
    "\n",
    "    n_windows = len(sample_x) // (window_len - window_overlap) - 1\n",
    "    window_data = []\n",
    "\n",
    "    fig, axs = plt.subplots(figsize=(20, 3), ncols=5, gridspec_kw={'width_ratios': [4,1,1,1,1]})\n",
    "    ax1 = axs[0]\n",
    "    ax1.scatter(sample_x, sample_y, s=32, c='grey', zorder=10, alpha=0.8)\n",
    "    #plt.vlines(x[idx], 0, y[idx], ls='--', color='grey')\n",
    "    ax1.plot(x, y)\n",
    "    ax1.set_xlabel('time (s)')\n",
    "    ax1.set_ylabel('amplitude')\n",
    "    for i in range(n_windows):\n",
    "        start = i * (window_len - window_overlap)\n",
    "        end = start + window_len\n",
    "        x_lb, x_ub = sample_x[start], sample_x[end]\n",
    "        # for better window visualisation\n",
    "        xx = np.linspace(x_lb, x_ub, 100)\n",
    "        w_vis = signal.get_window(window, 100)\n",
    "        ax1.plot(xx, w_vis, c='orange')\n",
    "        # the real window\n",
    "        w = signal.get_window(window, window_len)\n",
    "        window_data.append(w * sample_y[start:end])\n",
    "    #    ax.vlines(sample_x[start:end], sample_y[start:end], w, ls='--', color='grey', alpha=0.1)\n",
    "    ax1.set_title('Input signal and windows')\n",
    "\n",
    "    for i, ax in enumerate(axs.flat[1:]):\n",
    "        start = i * (window_len - window_overlap)\n",
    "        end = start + window_len\n",
    "        xx = np.arange(window_len)\n",
    "        ax.scatter(xx, sample_y[start:end], c='grey', s=64, label='original')\n",
    "        ax.scatter(xx, window_data[i], c='C1', marker='v', s=24, alpha=0.75, label='weighted')\n",
    "        ax.set_title(f'Data for window {i}')\n",
    "        ax.set_ylim(-1.1, 1.1)\n",
    "        ax.set_xlabel('n')\n",
    "        if i == 0:\n",
    "            ax.legend(loc='lower left')\n",
    "\n",
    "    for ax in axs:\n",
    "        ax.axhline(0, c='k', lw=1)\n",
    "    plt.show()\n",
    "\n",
    "    widgets.RadioButtons(\n",
    "    options=['pepperoni', 'pineapple', 'anchovies'],\n",
    "#    value='pineapple', # Defaults to 'pineapple'\n",
    "#    layout={'width': 'max-content'}, # If the items' names are long\n",
    "    description='Pizza topping:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "widgets.interact(plot,\n",
    "                 window=widgets.RadioButtons(options=['boxcar', 'hann', 'hamming', 'tukey'],\n",
    "                                            description='Window function',\n",
    "                                            value='hann'),\n",
    "                 window_len=widgets.IntSlider(value=9, min=9, max=15),\n",
    "                 window_overlap=widgets.IntSlider(value=3, min=0, max=8));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923728a6-d86d-4a97-ba5d-6aea30ea69b0",
   "metadata": {},
   "source": [
    "Here we show the true signal as the blue curve on the left and the sampled points in grey. On the same axis we plot the window weights (which are also on scale 0 to 1) as orange curves. For each window, the sample points that fall inside it get weighted according to the value of the orange curve at that x-coordinate. In the plots on the right we visualise the resulting weighted data as orange triangles for the first few windows. For comparison, the original unweighted data is also display in grey."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692dd760-3d36-48ad-9e16-494d5acc6e82",
   "metadata": {},
   "source": [
    "---\n",
    "### Spectrograms\n",
    "\n",
    "We finally get to the most indispensable tool in audio analysis: the **spectrogram**. This is what we get when we combine windowing and DFT. We start by splitting the audio signal into smaller segments according to our desired window specifications, then for each segment we apply the DFT in order to get its frequency content. The resulting data can be organised into a matrix so that each\n",
    "row corresponds to a specific frequency and columns to windows. The value of the $(i, j)$ cell is then the magnitude of the $i$-th frequency in the $j$-th window. This is easier to grasp if we visualise it as a heatmap.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6725fa0-e6ce-467c-8d17-175103bdb25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "duration = 10\n",
    "SEED = 42\n",
    "\n",
    "def plot(sample_rate, window, window_len, window_overlap_ratio):\n",
    "    window_overlap = int(window_overlap_ratio * window_len)\n",
    "    x = np.linspace(0, duration, num=int(duration * sample_rate))\n",
    "    y = np.sum(np.sin(2 * np.pi * C_freqs.T * x), axis=0) / C_freqs.shape[1]\n",
    "    rng = np.random.default_rng(SEED)\n",
    "    y = np.clip(rng.normal(size=len(y)) + y, -1, 1)\n",
    "\n",
    "    f, t, Sxx = signal.spectrogram(y, sample_rate, window=window, nperseg=window_len,\n",
    "                                   noverlap=window_overlap, mode='magnitude')\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    p = ax.pcolormesh(t, f, Sxx, cmap='magma', shading='auto',\n",
    "                     vmin=0, vmax=0.03)\n",
    "    ax.set_ylim(0, 2000)\n",
    "    ax.set_xlabel('Time (s)')\n",
    "    ax.set_ylabel('Frequency (Hz)')\n",
    "    fig.colorbar(p)\n",
    "    plt.show()\n",
    "\n",
    "widgets.interact(plot,\n",
    "                 sample_rate=widgets.Dropdown(options=[28000, 41000, 96000]),\n",
    "                 window=widgets.RadioButtons(options=['boxcar', 'hann', 'hamming', 'tukey'],\n",
    "                                            description='Window function',\n",
    "                                            value='hann'),\n",
    "                 window_len=widgets.IntSlider(value=2048, min=512, max=4096),\n",
    "                 window_overlap_ratio=widgets.FloatSlider(value=0.25, min=0, max=0.9));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039cbeb0-f8fe-4c0b-a210-b5d93ed5c968",
   "metadata": {},
   "source": [
    "Above we plot the spectrogram for the same signal of the C major as before, except this time we add some random noise to it to make the example a bit more interesting. We've also extended the signal to 10 seconds. The window overlap parameter is presented as a fraction of the window length. According to our earlier discussion, the intensity of the colour of each pixel in the spectrogram signifies how much of that particular frequency is present in the signal at given time (or more precisely in the corresponding window of the original sampled signal). Hence, we should expect to see three straight lines at the C major frequencies (roughly: 262 Hz, 330 Hz, 392 Hz) on top of the darker noisy background.\n",
    "\n",
    "> ðŸ”Š **Exercise**: Experiment with various FFT parameters. Try increasing the sampling rate. Why does this seem to make things worse sometimes? How and why is this related to the window length? Think about how the window length affects the frequency resolution (hint: recall the Nyquist frequency), i.e. how sharp the bright lines appear.\n",
    "\n",
    "> âš ï¸ **Note**: For real-world signals, it is usually desirable to transform the spectrogram to the **decibel scale**. For appropriately scaled signals this amounts to some constant multiple of the logarithm of the spectrogram. This is to approximate how humans perceive sound in that a doubling of the power of some frequency component results in a linear increase of roughly 3 dB for that frequency. You can also use `librosa.amplitude_to_db()`, [(docs)](https://librosa.org/doc/latest/generated/librosa.amplitude_to_db.html), to do the conversion.\n",
    "\n",
    "> ðŸ”Š **Exercise**: As a final exercise, try to apply the methods & code from this notebook to some real-world audio data instead of a synthetic signal. You can use `scipy.io.wavfile.read()`, [(docs)](https://docs.scipy.org/doc/scipy/reference/generated/scipy.io.wavfile.read.html), to read in a wav file to a numpy array. Note that wavs come in many different formats, in particular, the values can be either floats in [-1, 1) or ints of a given bit depth. If you want to be consistent in your analysis, you should scale all your files to the same range. One good program to help with investigating and converting audio files is [SoX](https://sox.sourceforge.net)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ff728e-3171-48dc-8b3a-3b94dd3a065a",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lesson we learnt how sound from the real-world is translated into digital format via a **sampling process**. This necessarily results in a loss of information, but we can minimise its effect by understanding the concepts of **Nyquist frequency** and **Nyquist rate**. We understand that sound is a very complex signal and thus it requires extremely fine-grained data (sampled at a rate of tens of thousands of samples per second). The resulting sequence is tricky to analyse directly and hence we employ a **discrete Fourier transform** in order to separate the different frequency components of the sequence. For longer audio files it is useful to split it into (possibly overlapping) segments where the values inside each segment are weighted according to a **window function**. We can then perform the DFT on each of these segments separately to obtain a **spectrogram**, which shows how the strength of each frequency changes over time. The spectrogram is the most fundamental building block of audio analysis and is usually the first thing to investigate when performing exploratory analysis of sound.\n",
    "\n",
    "## Where to go from here?\n",
    "\n",
    "Most audio analysis happens based on the spectrogram (i.e. in the **spectral domain**). There are many useful features you might want to compute based on it. For example, computing the overall **energy** of the signal is easy (think about why it's difficult to estimate it based on the raw signal, i.e. in the **time domain**), or you might be interested around which frequency most of that energy concentrates. These are all called **spectral features** and you can more examples in the [librosa documentation](https://librosa.org/doc/main/feature.html#spectral-features).\n",
    "\n",
    "We leave experimentation with such features to the reader and instead dive deeper into the theory of an important characteristic of sound (or waves or oscillating objects in general) which is **harmonic frequencies** and **resonance**. This is a crucial component for successfully analysing e.g. human speech (in particular for distinguishing vowels) or detecting faults in machinery (how to detect a failing gearbox).\n",
    "\n",
    "See you in part 2!\n",
    "\n",
    "## Further reading\n",
    "\n",
    "- [Introduction to Speech Processing - Aalto University](https://speechprocessingbook.aalto.fi)\n",
    "- [musicinformationretrieval.com](https://musicinformationretrieval.com/index.html)\n",
    "- [Fundamental of Music Processing notebooks - Meinard MÃ¼ller](https://www.audiolabs-erlangen.de/resources/MIR/FMP/C0/C0.html)\n",
    "- [MIT Course on Digital Signal Processing - Alan Oppenheim](https://www.youtube.com/playlist?list=PLLNp7XoiSLQZfrELgY1sCpAxYrvK55lbW)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
